@article{perez2021pysentimiento,
      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},
      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},
      year={2021},
      eprint={2106.09462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}, 
@online{BERTSearch,
  author = {Pandu Nayak},
  title = {{Understanding searches better than ever before}},
  year = 2019,
  url = {https://www.blog.google/products/search/search-language-understanding-bert/},
  urldate = {2019-10-25}
}, 
@online{BERTDifferences,
  author = {Lavanya Gupta},
  title = {{Differences Between Word2Vec and BERT}},
  year = {2020},
  url = {https://medium.com/swlh/differences-between-word2vec-and-bert-c08a3326b5d1#:~:text=Word2Vec%20will%20g},
  urldate = {2020-11-12}
},
@online{Transformers1,
  title = {{Differences Between Word2Vec and BERT}},
  year = 2021,
  author = {Dale Markowitz},
  url = {https://daleonai.com/transformers-explained},
  urldate = {2021-05-06}
}, 
@mastersthesis{BERTModel,
  author  = "Iago Collarte González",
  title   = "Procesamiento del lenguaje natural
con BERT: Análisis de sentimientos en tuits",
  school  = "Universidad Carlos 3 de Madrid",
  year    = "2020" 
}, 
@online{FillMask,
  title = {{Fill-Mask - Huggingface}},
  author = {HuggingFace}, 
  year = 2022,
  url = {https://huggingface.co/tasks/fill-mask}
},
@online{BERTExplained,
  author = {Samia Khalid},
  title = {{BERT Explained a complete guide}},
  year = 2017,
  url = {https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c}
},
@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
},
@article{adhikari2019docbert,
  title={Docbert: Bert for document classification},
  author={Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
  journal={arXiv preprint arXiv:1904.08398},
  year={2019}
},
@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7464--7473},
  year={2019}
},
@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
},
@article{shang2019pre,
  title={Pre-training of graph augmented transformers for medication recommendation},
  author={Shang, Junyuan and Ma, Tengfei and Xiao, Cao and Sun, Jimeng},
  journal={arXiv preprint arXiv:1906.00346},
  year={2019}
},
@online{patentBERT,
  author = {Dan Ofer},
  year = {2022},
  title = {{patentBERT Explained}},
  url = {https://www.kaggle.com/datasets/danofer/patentbert}
},
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
},
@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
},
@online{DistilBERT,
  title = {{DistilBERT}},
  author = {HuggingFace},
  year = {2021},
  url = {https://huggingface.co/docs/transformers/model_doc/distilbert}
}, 
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}, 
@mastersthesis{bautista2019analisis,
  title={An{\'a}lisis, implementaci{\'o}n y evaluaci{\'o}n de modelos de aprendizaje autom{\'a}tico relacional},
  author={Bautista Ulcuango, Alexis Enrique},
  type={{B.S.} thesis},
  year={2019},
  school={Quito: UCE}
}, 
@online{ElmoBert,
  author = {Ryan Burke},
  year = {2021},
  title = {{GloVe, ELMo y BERT. A guide to state-of-the-art text classification using Spark NLP}},
  url = {https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934},
  urldate = {2021-03-16}
}, 
@online{SEOBert,
  author = {InboundCycle},
  year = {2019},
  title = {{Análisis
profundo
de Google
BERT para
responsables
de marketing}},
  url = {https://cdn2.hubspot.net/hubfs/136661/1-Content/offer/An%C3%A1lisis%20Google%20BERT%20para%20responsables%20de%20marketing/Google%20BERT%20para%20responsables%20de%20marketing.pdf}
}, 
@online{BERTNLP,
  author = {Jay Alammar},
  year = {2021},
  title = {{The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)}},
  url = {https://jalammar.github.io/illustrated-bert/}
},
@online{word2Vec,
  author = {Eligijus Bujokas},
  year = {2020},
  title = {{Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning)}},
  url = {https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8}
}